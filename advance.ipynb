{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorboard\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils as utils\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset path\n",
    "data_path_train = \"data/training\"\n",
    "data_path_test = \"data/testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1646\n",
      "    Root location: data/training\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n",
      "{'Baked Potato': 0, 'Crispy Chicken': 1, 'Donut': 2, 'Fries': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/NFS/course/dl2023f/dl2023f_087/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# data transform, you can add different transform methods and resize image to any size\n",
    "img_size = 224\n",
    "transform = transforms.Compose([\n",
    "                       transforms.Resize((img_size,img_size)),\n",
    "                       transforms.ToTensor()\n",
    "                       ])\n",
    "\n",
    "#build dataset\n",
    "dataset = datasets.ImageFolder(root=data_path_train,transform=transform)\n",
    "\n",
    "# spilt your data into train and val\n",
    "TOTAL_SIZE = len(dataset)\n",
    "ratio = 0.9\n",
    "train_len = round(TOTAL_SIZE * ratio)\n",
    "valid_len = round(TOTAL_SIZE * (1-ratio))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, valid_len])\n",
    "\n",
    "#build dataloader\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True,  num_workers=4)\n",
    "val_data_loader = data.DataLoader(val_dataset, batch_size=32, shuffle=True,  num_workers=4)\n",
    "\n",
    "#check dataset\n",
    "print(dataset)\n",
    "print(dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def train(model, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Iterate over data\n",
    "    for inputs, labels in train_data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.sum(preds == labels.data)\n",
    "        \n",
    "    avg_loss = total_loss / len(train_data_loader)\n",
    "    accuracy = total_correct.double() / len(train_dataset) * 100\n",
    "\n",
    "    print('Training Accuracy: {:.4f}% Training Loss: {:.4f}'.format(accuracy, avg_loss))\n",
    "    return \n",
    "\n",
    "#validation function\n",
    "def valid(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Iterate over data\n",
    "    for inputs, labels in val_data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.sum(preds == labels.data)\n",
    "        \n",
    "    avg_loss = total_loss / len(val_data_loader)\n",
    "    accuracy = total_correct.double() / len(val_dataset) * 100\n",
    "\n",
    "    print('Validation Accuracy: {:.4f}% Validation Loss: {:.4f}'.format(accuracy, avg_loss))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/NFS/course/dl2023f/dl2023f_087/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/NFS/course/dl2023f/dl2023f_087/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/NFS/course/dl2023f/dl2023f_087/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# using gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#build your model here\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------start training--------------\n",
      "epoch: 1\n",
      "Training Accuracy: 27.4814% Training Loss: 2.4538\n",
      "Validation Accuracy: 29.6970% Validation Loss: 5.8419\n",
      "model saved\n",
      "epoch: 2\n",
      "Training Accuracy: 36.5969% Training Loss: 1.5520\n",
      "Validation Accuracy: 37.5758% Validation Loss: 2.7976\n",
      "model saved\n",
      "epoch: 3\n",
      "Training Accuracy: 38.3525% Training Loss: 1.3528\n",
      "Validation Accuracy: 37.5758% Validation Loss: 3.4826\n",
      "epoch: 4\n",
      "Training Accuracy: 42.8089% Training Loss: 1.3337\n",
      "Validation Accuracy: 47.2727% Validation Loss: 1.3077\n",
      "model saved\n",
      "epoch: 5\n",
      "Training Accuracy: 51.3167% Training Loss: 1.1407\n",
      "Validation Accuracy: 44.2424% Validation Loss: 1.1752\n",
      "epoch: 6\n",
      "Training Accuracy: 56.1783% Training Loss: 1.0614\n",
      "Validation Accuracy: 42.4242% Validation Loss: 2.9637\n",
      "epoch: 7\n",
      "Training Accuracy: 61.0398% Training Loss: 0.9945\n",
      "Validation Accuracy: 60.6061% Validation Loss: 0.9770\n",
      "model saved\n",
      "epoch: 8\n",
      "Training Accuracy: 59.5544% Training Loss: 1.0149\n",
      "Validation Accuracy: 59.3939% Validation Loss: 0.9125\n",
      "epoch: 9\n",
      "Training Accuracy: 61.1749% Training Loss: 0.9663\n",
      "Validation Accuracy: 48.4848% Validation Loss: 1.2462\n",
      "epoch: 10\n",
      "Training Accuracy: 61.7826% Training Loss: 0.9556\n",
      "Validation Accuracy: 57.5758% Validation Loss: 1.0853\n",
      "epoch: 11\n",
      "Training Accuracy: 63.4706% Training Loss: 0.9461\n",
      "Validation Accuracy: 51.5152% Validation Loss: 1.1214\n",
      "epoch: 12\n",
      "Training Accuracy: 67.0493% Training Loss: 0.8328\n",
      "Validation Accuracy: 60.0000% Validation Loss: 0.9018\n",
      "epoch: 13\n",
      "Training Accuracy: 71.2357% Training Loss: 0.7928\n",
      "Validation Accuracy: 44.2424% Validation Loss: 1.5906\n",
      "epoch: 14\n",
      "Training Accuracy: 70.6280% Training Loss: 0.7980\n",
      "Validation Accuracy: 41.8182% Validation Loss: 1.5136\n",
      "epoch: 15\n",
      "Training Accuracy: 69.5476% Training Loss: 0.7550\n",
      "Validation Accuracy: 47.8788% Validation Loss: 1.9941\n",
      "epoch: 16\n",
      "Training Accuracy: 69.6151% Training Loss: 0.8133\n",
      "Validation Accuracy: 56.3636% Validation Loss: 1.1768\n",
      "epoch: 17\n",
      "Training Accuracy: 72.1810% Training Loss: 0.7349\n",
      "Validation Accuracy: 32.1212% Validation Loss: 2.8497\n",
      "epoch: 18\n",
      "Training Accuracy: 73.2613% Training Loss: 0.7447\n",
      "Validation Accuracy: 54.5455% Validation Loss: 1.1920\n",
      "epoch: 19\n",
      "Training Accuracy: 73.8690% Training Loss: 0.7255\n",
      "Validation Accuracy: 55.1515% Validation Loss: 1.0451\n",
      "epoch: 20\n",
      "Training Accuracy: 75.2194% Training Loss: 0.6738\n",
      "Validation Accuracy: 61.8182% Validation Loss: 1.0026\n",
      "model saved\n",
      "epoch: 21\n",
      "Training Accuracy: 80.2836% Training Loss: 0.5958\n",
      "Validation Accuracy: 58.1818% Validation Loss: 1.1952\n",
      "epoch: 22\n",
      "Training Accuracy: 76.3673% Training Loss: 0.6128\n",
      "Validation Accuracy: 67.2727% Validation Loss: 0.9415\n",
      "model saved\n",
      "epoch: 23\n",
      "Training Accuracy: 77.3126% Training Loss: 0.6087\n",
      "Validation Accuracy: 71.5152% Validation Loss: 0.6805\n",
      "model saved\n",
      "epoch: 24\n",
      "Training Accuracy: 80.5537% Training Loss: 0.5352\n",
      "Validation Accuracy: 56.3636% Validation Loss: 1.1737\n",
      "epoch: 25\n",
      "Training Accuracy: 79.6084% Training Loss: 0.5641\n",
      "Validation Accuracy: 66.6667% Validation Loss: 0.8700\n",
      "epoch: 26\n",
      "Training Accuracy: 80.4862% Training Loss: 0.5301\n",
      "Validation Accuracy: 62.4242% Validation Loss: 1.3356\n",
      "epoch: 27\n",
      "Training Accuracy: 83.5922% Training Loss: 0.4498\n",
      "Validation Accuracy: 60.0000% Validation Loss: 1.5936\n",
      "epoch: 28\n",
      "Training Accuracy: 80.8238% Training Loss: 0.5469\n",
      "Validation Accuracy: 71.5152% Validation Loss: 1.1169\n",
      "epoch: 29\n",
      "Training Accuracy: 81.2964% Training Loss: 0.5203\n",
      "Validation Accuracy: 64.8485% Validation Loss: 1.1514\n",
      "epoch: 30\n",
      "Training Accuracy: 82.9845% Training Loss: 0.4760\n",
      "Validation Accuracy: 58.7879% Validation Loss: 1.0833\n",
      "epoch: 31\n",
      "Training Accuracy: 86.6982% Training Loss: 0.3698\n",
      "Validation Accuracy: 53.9394% Validation Loss: 1.8310\n",
      "epoch: 32\n",
      "Training Accuracy: 83.9973% Training Loss: 0.4264\n",
      "Validation Accuracy: 67.8788% Validation Loss: 0.8855\n",
      "epoch: 33\n",
      "Training Accuracy: 85.2802% Training Loss: 0.4171\n",
      "Validation Accuracy: 74.5455% Validation Loss: 0.7979\n",
      "model saved\n",
      "epoch: 34\n",
      "Training Accuracy: 87.7785% Training Loss: 0.3466\n",
      "Validation Accuracy: 60.6061% Validation Loss: 1.9603\n",
      "epoch: 35\n",
      "Training Accuracy: 86.5631% Training Loss: 0.3674\n",
      "Validation Accuracy: 66.0606% Validation Loss: 1.4978\n",
      "epoch: 36\n",
      "Training Accuracy: 86.2255% Training Loss: 0.3996\n",
      "Validation Accuracy: 49.6970% Validation Loss: 2.1272\n",
      "epoch: 37\n",
      "Training Accuracy: 88.4537% Training Loss: 0.3013\n",
      "Validation Accuracy: 61.2121% Validation Loss: 1.2336\n",
      "epoch: 38\n",
      "Training Accuracy: 90.5469% Training Loss: 0.2760\n",
      "Validation Accuracy: 80.0000% Validation Loss: 0.7194\n",
      "model saved\n",
      "epoch: 39\n",
      "Training Accuracy: 89.2640% Training Loss: 0.3049\n",
      "Validation Accuracy: 81.2121% Validation Loss: 0.5098\n",
      "model saved\n",
      "epoch: 40\n",
      "Training Accuracy: 86.4956% Training Loss: 0.3284\n",
      "Validation Accuracy: 64.8485% Validation Loss: 1.9844\n",
      "epoch: 41\n",
      "Training Accuracy: 89.4666% Training Loss: 0.2705\n",
      "Validation Accuracy: 68.4848% Validation Loss: 1.2569\n",
      "epoch: 42\n",
      "Training Accuracy: 90.8170% Training Loss: 0.2409\n",
      "Validation Accuracy: 64.8485% Validation Loss: 1.4240\n",
      "epoch: 43\n",
      "Training Accuracy: 92.0324% Training Loss: 0.2189\n",
      "Validation Accuracy: 66.6667% Validation Loss: 1.1756\n",
      "epoch: 44\n",
      "Training Accuracy: 91.8974% Training Loss: 0.2330\n",
      "Validation Accuracy: 81.2121% Validation Loss: 0.6511\n",
      "epoch: 45\n",
      "Training Accuracy: 92.6401% Training Loss: 0.2006\n",
      "Validation Accuracy: 72.1212% Validation Loss: 0.9817\n",
      "epoch: 46\n",
      "Training Accuracy: 92.9102% Training Loss: 0.1923\n",
      "Validation Accuracy: 79.3939% Validation Loss: 0.8230\n",
      "epoch: 47\n",
      "Training Accuracy: 92.5726% Training Loss: 0.2090\n",
      "Validation Accuracy: 76.3636% Validation Loss: 0.7183\n",
      "epoch: 48\n",
      "Training Accuracy: 93.3153% Training Loss: 0.1972\n",
      "Validation Accuracy: 54.5455% Validation Loss: 1.9363\n",
      "epoch: 49\n",
      "Training Accuracy: 89.9392% Training Loss: 0.2530\n",
      "Validation Accuracy: 73.3333% Validation Loss: 1.4155\n",
      "epoch: 50\n",
      "Training Accuracy: 95.4760% Training Loss: 0.1305\n",
      "Validation Accuracy: 78.7879% Validation Loss: 0.7715\n"
     ]
    }
   ],
   "source": [
    "####################  implement your optimizer ###################################\n",
    "## you can use any training methods if you want (ex:lr decay, weight decay.....)\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# start training\n",
    "model.to(device=device)\n",
    "acc_best = 0.0\n",
    "\n",
    "print('--------------start training--------------')\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    print('epoch:', epoch)\n",
    "    train(model, criterion, optimizer)\n",
    "    accuracy = valid(model, criterion)\n",
    "    writer.add_scalar('Learning_rate', learning_rate, epoch)\n",
    "    writer.add_histogram('Weights', model.conv1.weight, epoch)\n",
    "    writer.add_histogram('Gradients', model.conv1.weight.grad, epoch)\n",
    "    if accuracy > acc_best:\n",
    "        acc_best = accuracy\n",
    "        print(\"model saved\")\n",
    "        torch.save(model, \"model.pth\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.ToTensor()\n",
    "                                    ])\n",
    "\n",
    "dataset_test = datasets.ImageFolder(root=data_path_test, transform=transform_test)\n",
    "dataloader_test  = data.DataLoader(dataset_test, batch_size=8, shuffle=False, num_workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model so that you don't need to train the model again\n",
    "test_model = torch.load(\"model.pth\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        bs = dataloader_test.batch_size\n",
    "        result = []\n",
    "        for i, (data, target) in enumerate(dataloader_test):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, 1, keepdim=True)\n",
    "            \n",
    "            arr = preds.data.cpu().numpy()\n",
    "            for j in range(preds.size()[0]):\n",
    "                file_name = dataset_test.samples[i*bs+j][0].split('/')[-1]\n",
    "                result.append((file_name,preds[j].cpu().numpy()[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('ID_result.csv','w') as f:\n",
    "    f.write('ID,label\\n')\n",
    "    for data in result:\n",
    "        f.write(data[0]+','+str(data[1])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
